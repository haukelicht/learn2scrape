---
title: "Introduction to Webscraping"
output: 
  learnr::tutorial:
    progressive: false
runtime: shiny_prerendered
---

```{r setup, include=FALSE}
library(learnr)
library(learn2scrape)
library(rvest)
knitr::opts_chunk$set(echo = FALSE)
quotepage<-system.file("extdata", "quotepage.html", package = "learn2scrape")
```

## Welcome

*Last revised: `r Sys.Date()`*

Welcome to webscraping with learn2scrape!  This is a first attempt at making this class more interactive and fun - if you have any feedback, let me know in the next days [or after the course via e-mail](mailto:gessler@ipz.uzh.ch)! Also, if you are interested in scraping, feel free to [watch the repository on Github](https://github.com/theresagessler/learn2scrape) to get updates on new elements.

The tutorial has explanations but there are many interactive parts where you will be asked to write code. Some notes about the format:

- If you don't know the answer, you can click on *Hint* (where available) or the *Solution* Button to see the solution 
    - or ask me or your neighbours for help, asking makes everything better!
- You can test your code by clicking on *Run Code*
- Since there is no environment like in RStudio, you have to call the objects to inspect their content or use `ls()` to see what is in your environment when you execute the chunk
- Be cautious about the *Start over* Button: Clicking on it will erase the code you wrote

Unlike in a normal markdown document, all chunks are independent. That means: If you create something in one chunk, it won't be available in the other chunks. Therefore, you will sometimes have to repeat code from previous exercises but I have tried to keep this to a minimum. Either use it as an opportunity to repeat or copy-paste - learnr saves your answers.

*Also, some of the exercises will use [piping](https://magrittr.tidyverse.org/) since it just makes scraping so much easier - if you are not familiar with this or any other element of the code, just let me know.*

<!--If you prefer using [swirl](https://swirlstats.com/){target="_blank"}, this first lesson is also available as a swirl-course. As things get more complex later on, I'll ask you to switch to the browser-version! -->

## Basics of webscraping

We start with something very simple: reading web data into R. Imagine, we want to scrape a simple webpage full of quotes. Its address is [http://quotes.toscrape.com/](http://quotes.toscrape.com/){target="_blank"}. Just have a look at the webpage.

- **First**, we need to load the package we'll use for most of our scraping. It is called *rvest*. Please load it with the `library()` command.
- **Next**, to read the page into R, we need to tell R its address - please create a character vector named `url` that contains the URL http://quotes.toscrape.com/
    - You can see if it worked by calling the object `url`.
- read in (sometimes called: parse) the webpage. To tell R to read the webpage, we can use the function `read_html()`. Use this on the url object we just created!

```{r read, exercise=TRUE,exercise.lines=3}
"http://quotes.toscrape.com/"
```

```{r read-solution}
library(rvest)
url<-"http://quotes.toscrape.com/"
read_html(url)
```

The output does not look like you expected? Actually, the [original webpage](http://quotes.toscrape.com/){target="_blank"} looks quite similar. Have a look at the webpage source code. Depending on your browser, you can probably select 'view source' after a right mouse click. If you have trouble, [google has an up-to-date explanation](https://www.google.com/search?hl=&site=&q=how+to+view+webpage+source+code+in+browser){target="_blank"}. 

*Just if you were wondering, of course, we could have also skipped creating a variable with the URL. and could have instead directly applied the `read_html()` function  to the URL http://quotes.toscrape.com/.*

## Extracting elements


The function `read_html()` parses the html code, similar to what our browser does. Still, it gives us the entire document including the HTML commands. Since we do not want the formatting of the webpage, we can use the function html_text() to extract the Webpage text.

Try it out: assign the parsed page to an object called page and apply `html_text()` to it.




```{r text, exercise=TRUE,exercise.lines=2}

```

```{r text-solution}
page<-read_html("http://quotes.toscrape.com/")
html_text(page)
```

Did you find the quotes from before? Admittedly, this still looks very messy. 
Maybe you are thinking: If only, there would be a way to tell R to just get the text of the quotes! Luckily there is.

### CSS Selectors

The html_nodes() command allows us to select specific 'nodes', that is, elements of the HTML Code. Please have a look at the documentation of the html_nodes() command.

```{r documentation,exercise=TRUE}
library(rvest)
?html_nodes
```

So, we need xpath or CSS selectors. If you have not used HTML before, this might sound complicated. It helps to get a bit into the structure of HTML. [Click on this link to read an introduction to HTML](https://www.w3schools.com/html/default.asp){target="_blank"}.

Basically, HTML Tags describe the formatting and structure of a webpage. CSS selectors are a type of *grammar* or *pattern-description* that helps us select specific parts of that structure. I am working on a tutorial specific to CSS selectors (that you will be able to access by running `run_tutorial("CSS",package="learn2scrape")` in the future - right now it is just a list), but for now, we will just use a tool that helps us determine the correct selectors. But that is not a problem: many people use these tools for scraping and only learn the basics of CSS selectors.

For now, we will focus on two of the most important selectors: In principle, selectors work on tags - so if you write the name of a tag, the CSS selector will select all elements with that tag. **Try it out with some of the HTML tags that we learned on the slides.** Another useful selector is the star-symbol - it just selects *all* tags in the page (so it is a universal selector). As I said, we will focus more on gathering specific information but if you just want to parse large amounts of data, that can be very useful. **Try the universal selector on our webpage.**

```{r selectors,exercise=TRUE, exercise.lines=5}
page<-read_html("http://quotes.toscrape.com/")


```

```{r selectors-solution}
page<-read_html("http://quotes.toscrape.com/")
page %>% html_nodes("a")
page %>% html_nodes("h1")
page %>% html_nodes("*")
```

And for reference: For a list of CSS Selectors, check out [this collection](https://www.w3schools.com/cssref/css_selectors.asp){target="_blank"}. If you want to practice CSS Selectors in a fun way, I recommend playing with the [CSS Diner](https://flukeout.github.io/){target="_blank"} where you can learn about different selector structures.

## Extracting elements with SelectorGadget

While understanding HTML helps, we often do not need to engage with the code because there are lots of tools to help us. For example, SelectorGadget is a JavaScript bookmarklet that allows you to interactively figure out what css selector you need to extract parts of the page. If you have not heard of selectorgadget, check its [webpage](https://selectorgadget.com/){target="_blank"} - there is even a video:

![](https://vimeo.com/52055686)

We will try to use SelectorGadget now. [If you have Chrome, you can just install SelectorGadget in your browser](https://chrome.google.com/webstore/detail/selectorgadget/mhjhnkcfbdhnjickkkdbjoemdmbfginb). If you have a different browser, drag this link into your [bookmark bar and click on it when needed](javascript:(function(){var%20s=document.createElement('div');s.innerHTML='Loading...';s.style.color='black';s.style.padding='20px';s.style.position='fixed';s.style.zIndex='9999';s.style.fontSize='3.0em';s.style.border='2px%20solid%20black';s.style.right='40px';s.style.top='40px';s.setAttribute('class','selector_gadget_loading');s.style.background='white';document.body.appendChild(s);s=document.createElement('script');s.setAttribute('type','text/javascript');s.setAttribute('src','https://dv0akt2986vzh.cloudfront.net/unstable/lib/selectorgadget.js');document.body.appendChild(s);})();).
Now, use it to select all quotes on the quotes webpage we have used.

1. Click on the element you want to select. SelectorGadget will make a first guess at what css selector you want and mark all similar elements. It's likely to be a bad guess since it only has one example to learn from, but it's a start. Elements that match the selector will be highlighted in yellow.
2. Click on elements that shouldn't be selected. They will turn red.  Click on elements that *should* be selected but are not so far. They will turn green.
3. Iterate until only the elements you want are selected.  SelectorGadget is not perfect and sometimes will not be able to find a useful css selector. Sometimes starting from a different element helps.

What is the selector you receive?

```{r selector-practice, echo=FALSE}
quiz(caption= "Quiz about selectors",
question("Try finding the CSS selector for the text of the quote, without author and tags. What is the selector you receive?",
  answer(".quote",message="Almost but we did not want to include the author and tags!"),
  answer(".tags .tag"),
  answer(".text", correct = TRUE),
  answer("h2"),
  allow_retry=TRUE
),
question("Try finding the CSS selector for all tags associated with each quote. Deselect the Top Ten tags on the side. What is the selector you receive?",
  answer(".quote"),
  answer(".tags .tag", correct = TRUE),
  answer(".text"),
  answer("h2"),
  allow_retry=TRUE
))
```


### Applying CSS Selectors

Now, we try to use these CSS Selectors with the `html_nodes()` command. Since each exercise chunk is independent, there will be a bit of repetition involved but it aids the memory: Parse the page, use the CSS selector to select only the quotes from the parsed HTML and assign them to a new object `selected_nodes`. Then, inspect the results by calling the object!



```{r nodes, exercise=TRUE,exercise.setup="page-setup"}

```

```{r nodes-solution,exercise.setup='page-setup'}
page<-read_html("http://quotes.toscrape.com/")
selected_nodes<-html_nodes(page,".text")
selected_nodes
```

This already looks more structured - but we should get rid of the HTML tags. Try applying the `html_text()` command we used before to the nodes which we selected in the last step. This way, we get just the text from the nodes we selected. You can copy the code you used to extract the nodes and continue working on that!

```{r textnodes, exercise=TRUE, exercise.setup='page-setup'}


```

```{r textnodes-solution,exercise.setup='page-setup'}
page<-read_html("http://quotes.toscrape.com/")
selected_nodes<-html_nodes(page,".text")
selected_nodes %>% html_text()
```

## Wrap-up

**Congratulations! If you completed this lesson, you scraped your first webpage! That was not so bad, was it?**

To continue learning webscraping, you can run the following lessons from your R console:


<!--- Scraping files: run_tutorial("Files",package="learn2scrape")-->
<!-- - Learning CSS Selectors: run_tutorial("CSS",package="learn2scrape")-->
- Scraping multiple pages (more of the essentials): 
    - `run_tutorial("scraping_pages",package="learn2scrape")`
- Short section on scraping tables from webpages (do this if you think it would be useful for you): 
    - `run_tutorial("tables",package="learn2scrape")`
- Short section on scraping files (do this if you think it would be useful for you): 
    - `run_tutorial("files",package="learn2scrape")`
- EPSA conference scraping project (more advanced and complicated but we will use the data tomorrow): 
    - `run_tutorial("scraping_epsa",package="learn2scrape")`

