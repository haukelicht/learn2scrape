---
title: "Scraping Tables"
author: "Theresa Gessler"
output: learnr::tutorial
runtime: shiny_prerendered
---


```{r setup, include=FALSE}
library(learnr)
library(magrittr)
library(dplyr)
library(rvest)
knitr::opts_chunk$set(echo = FALSE)
```

### Introduction

*Last revised: `r Sys.Date()`*

Compared to text, tables are a lot easier to scrape: 

- Using `html_table()` we can just extract all tables contained in the page and manually select those that are relevant for us
- alternatively, we can use html_nodes to select specific tables

We will practice both. For this lesson, I have loaded the packages `rvest`, `magrittr` and `dplyr`.

### Scraping all tables

As mentioned, we use a command called `html_table()`.
The single most important specification for the commandis the `fill` parameter. If you specify fill as true inside the `html_table()` command, rvest will automatically fill rows with fewer than the maximum number of columns with NAs. This is useful because tables on the internet are often messy - they have inconsistent numbers of cells per row or the format is otherwise messed up. the fill specificaion allows you to deal with that by adding NA values.

Try it out on [wikipedia's list of the tallest buildings](https://en.wikipedia.org/wiki/List_of_tallest_buildings){target="_blank"}. Read the page and then apply the `html_table()` command with and without the specification.

```{r fill, exercise=TRUE}
"https://en.wikipedia.org/wiki/List_of_tallest_buildings"
```

```{r fill-solution}
read_html("https://en.wikipedia.org/wiki/List_of_tallest_buildings") %>% html_table()
read_html("https://en.wikipedia.org/wiki/List_of_tallest_buildings") %>% html_table(fill=T)
```

If you assign the result to an object, the object will be a list.
You can extract specific tables from this list by subsetting the list (that is, putting the number of the table you want in two squared brackets). Or, if you want to proceed in a piping-chain, you can use the command `extract2()`, adding the number of the table in brackets (the command name is no typo - `extract` without the 2 works for vectors, `extract2()` works for lists).

**Try both variants for extracting the second table from the list of tallest buildings that we scraped.**

```{r extract, exercise=TRUE}
"https://en.wikipedia.org/wiki/List_of_tallest_buildings"
```

```{r extract-solution}
tables<-read_html("https://en.wikipedia.org/wiki/List_of_tallest_buildings") %>% html_table(fill=T)
tables[[2]]
tables %>% extract2(2)
```

### Selecting specific tables

Alternatively, you can select specific tables upfront. This is for example useful when you scrape different pages that all contain the relevant information in a table but the tables are in a different order - think articles about artists or writers that all contain a list of their work but may also contain other optional tables beforehand.

In this case, you can just use `html_nodes()` to extract the relevant table if the tables share a common CSS selector. 

We practice this by downloading the basic information of [each member of the cast of the movie Wine Country](https://en.wikipedia.org/wiki/Wine_Country_(film)){target="_blank"} from wikipedia - that is the grey box you usually see [on the right of their page](https://en.wikipedia.org/wiki/Amy_Poehler){target="_blank"}.

Try the following:

- Use SelectorGadget to select all the links to the cast members from the movie page
    - extract the links to their pages
- extract the tables from all pages
    - write a function that uses the CSS selector of their information box
    - scrape them all, for example by writing a function and applying it with `lapply()` on the list of links

If you have trouble finding the CSS selectors, clicking on 'Hint' will show them to you. If you just want the solution, click on 'Next Hint' again.

```{r wine, exercise=TRUE}
"https://en.wikipedia.org/wiki/Wine_Country_(film)"

```
```{r wine-hint}
page<-read_html("https://en.wikipedia.org/wiki/Wine_Country_(film)") 
urls<-html_nodes(page,"tr:nth-child(7) a")

# CSS node for tables
node=".infobox.vcard"
```

```{r wine-solution,eval=FALSE}
page<-read_html("https://en.wikipedia.org/wiki/Wine_Country_(film)") 
urls<-html_nodes(page,"tr:nth-child(7) a") %>% html_attr("href")
urls<-paste0("https://en.wikipedia.org",urls)
scrape_tables<-function(urls,node=".infobox.vcard"){
  read_html(urls) %>% html_nodes(node) %>% html_table(fill=T) %>% extract2(1)
}
lapply(urls,scrape_tables)
```

Unfortunately, you will see that all tables are quite heterogeneous - so this is not a general purpose solution. Still, when you are  dealing with election results or similar data, scraping tables based on their nodes can be incredibly helpful!
