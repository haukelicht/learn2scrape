---
title: "Scraping Tables"
output: learnr::tutorial
runtime: shiny_prerendered
---


```{r setup, include=FALSE}
library(learnr)
library(magrittr)
library(dplyr)
library(rvest)
knitr::opts_chunk$set(echo = FALSE)
```

Compared to text, tables are a lot more satisfying to scrape: Using `html_table()` we can just extract all tables contained in the page and then select those that are relevant for us. Or, we can again use html_nodes to select specific tables. For this lesson, I have loaded the packages `rvest`, `magrittr` and `dplyr`.

### Scraping all tables

The single most important specification is the `fill` parameter. If you specify fill as true inside the `html_table()` command, rvest will automatically fill rows with fewer than the maximum number of columns with NAs. This is useful because it allows you to scrape messy tables with inconsistent numbers of cells per row.

Try it out on [wikipedia's list of the tallest buildings](https://en.wikipedia.org/wiki/List_of_tallest_buildings){target="_blank"}.

```{r fill, exercise=TRUE}
"https://en.wikipedia.org/wiki/List_of_tallest_buildings"
```

```{r fill-hint}
read_html("https://en.wikipedia.org/wiki/List_of_tallest_buildings") %>% html_table()
read_html("https://en.wikipedia.org/wiki/List_of_tallest_buildings") %>% html_table(fill=T)
```

If you assign the result to an object, the result will be a list.
You can extract specific tables from this list by subsetting the list, putting the number of the table you want in two squared brackets. Or, if you want to proceed in a piping-chain, you can use the command `extract2()`, adding the number of the table in brackets (the command name is no typo - `extract` without the 2 works for vectors, `extract2()` works for lists). Try both variants for extracting the second table from the list of tallest buildings that we scraped.

```{r extract, exercise=TRUE}
"https://en.wikipedia.org/wiki/List_of_tallest_buildings"
```

```{r extract-hint}
tables<-read_html("https://en.wikipedia.org/wiki/List_of_tallest_buildings") %>% html_table(fill=T)
tables[[2]]
tables %>% extract2(2)
```

### Selecting specific tables

Alternatively, you can select specific tables upfront. This is for example useful when you scrape different pages that all contain the relevant information but potentially in a different order - think articles about artists or writers that all contain a list of their work but may also contain other optional tables beforehand.

In this case, you can just use `html_nodes()` to extract the relevant table if the tables share a common CSS selector. 

We practice this by downloading the basic information of [each member of the cast of the movie Wine Country](https://en.wikipedia.org/wiki/Wine_Country_(film)){target="_blank"} from wikipedia - that is the grey box you usually see [on the right of their page](https://en.wikipedia.org/wiki/Amy_Poehler){target="_blank"}.

Try the following:

- Use SelectorGadget to select all the links to the cast members
- extract the links to their pages
- extract the tables from all pages
    - write a function that uses the CSS selector of their information box
    - scrape them all, for example by writing a function and applying it with `lapply()` on the list of links

If you have trouble finding the CSS selectors, clicking on 'Hint' will show them to you. If you just want the solution, click on 'Next Hint' again.

```{r wine, exercise=TRUE}
"https://en.wikipedia.org/wiki/Wine_Country_(film)"

```
```{r wine-hint}
page<-read_html("https://en.wikipedia.org/wiki/Wine_Country_(film)") 
urls<-html_nodes(page,"tr:nth-child(7) a")

# CSS node for tables
node=".infobox.vcard"
```

```{r wine-solution,eval=FALSE}
page<-read_html("https://en.wikipedia.org/wiki/Wine_Country_(film)") 
urls<-html_nodes(page,"tr:nth-child(7) a") %>% html_attr("href")
urls<-paste0("https://en.wikipedia.org",urls)
scrape_tables<-function(urls,node=".infobox.vcard"){
  read_html(urls) %>% html_nodes(node) %>% html_table(fill=T) %>% extract2(1)
}
lapply(urls,scrape_tables)
```

Unfortunately, you will see that all tables are quite heterogeneous - so this is not a general purpose solution. Still, when you are for example dealing with election results, scraping tables based on their nodes can be incredibly helpful!
