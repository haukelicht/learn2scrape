---
pagetitle: "Creating a dictionary for classification"
output: 
  learnr::tutorial:
    progressive: false
runtime: shiny_prerendered
---

```{r setup, include=FALSE}
library(learnr)
knitr::opts_chunk$set(echo = FALSE,message=F,warning=F)
load("epsa_paper.RData")
```

## Welcome

*Last revised: `r Sys.Date()`*

This is the tutorial for dictionary analysis. We will
For our analysis, we first need to load some packages: **quanteda** (for text analysis), **dplyr** (for data manipulation) and **ggplot2** (for visualizing our results). I have already pre-loaded the data frame we will use. It is called **epsa_paperwise**.

```{r libraries,exercise=T}
library(quanteda)
library(dplyr)
```

## Pre-processing data

### Pre-processing steps

As mentioned, there are several stages to the pre-processing process:
- creating a corpus: `corpus()`
- tokenizing the corpus: `tokens()`
- generating a document feature matrix: `dfm()`

```{r corpus, exercise=T,exercise.setup='packset'}

```

```{r corpus-hint,exercise=T}
?corpus
?tokens
?dfm
```

```{r corpus-solution}
library(quanteda)
epsa_corpus<-corpus(epsa_paperwise,text_field = "abstracts")
epsa_tokens<-tokens(epsa_corpus,remove_hyphens=T,remove_symbol=T,remove_punct=T)
epsa_dfm<-dfm(epsa_tokens) %>% dfm_wordstem(language="en") %>% dfm_remove(stopwords("en"))
```



## Loading an existing dictionary


```{r dictset,include=F}
epsa_corpus<-corpus(epsa_paperwise,text_field = "abstracts")
epsa_tokens<-tokens(epsa_corpus,remove_hyphens=T,remove_symbol=T)
epsa_dfm<-dfm(epsa_tokens) %>% dfm_wordstem(language="en") %>% dfm_remove(c(stopwords("en"),"-"))
```

Dictionaries come in different forms. For pre-defined dictionaries, YAML is one of of the most frequent formats. For our example, we will use the [newsmap dictionary](). You can find it in the folder data and the file is called newsmap.yml First, try to load the dictionary using the `dictionary` function. Check the documentation to see the syntax or ask for a hint.

```{r load-dict, exercise=T,exercise.setup='dictset',lines=3}
?dictionary


```

```{r load-dict-hint, exercise=T,exercise.setup='dictset'}
newsmap_dict <- dictionary(file = "data/newsmap.yml",
                           format = "YAML")

```



```{r load-dict-solution, exercise=T,exercise.setup='dictset'}
newsmap_dict <- dictionary(file = "data/newsmap.yml",
                           format = "YAML")

```


To view the dictionary, you can use the `print()` command

```{r print-dict,exercise=T,exercise.setup='dictset'}
print(newsmap_dict)
```


## Applying the dictionary

Dictionaries with a hierarchical structure can be evaluated at different levels. The newsmap dictionary has such a hierarchical structure: It contains countries, nested in regions which are then nested in continents.

```{r apply-dict,exercise=T,exercise.setup='dictset'}
region_dfm <- dfm_lookup(epsa_dfm, newsmap_dict, levels = 1)
textstat_frequency(region_dfm)
country_dfm <- dfm_lookup(epsa_dfm, newsmap_dict, levels = 3)
textstat_frequency(country_dfm)

textstat_frequency(region_dfm,groups="section")
textstat_frequency(region_dfm,groups="section") %>% ggplot(aes(y=docfreq,color=group,x=feature))+geom_point()
kwic_map <- kwic(epsa_tokens, newsmap_dict['AFRICA'])
kwic_map

dfm_group(region_dfm,"section") %>% dfm_weight("prop") %>% textstat_frequency(group="section",force=T) %>% ggplot(aes(y=frequency,color=group,x=feature))+geom_point()
```

```{r apply-dict-solution,exercise=T,exercise.setup='dictset'}
# apply one level of the dictionary (continent)
region_dfm <- dfm_lookup(epsa_dfm, newsmap_dict, levels = 1)
textstat_frequency(region_dfm)
country_dfm <- dfm_lookup(epsa_dfm, newsmap_dict, levels = 3)
textstat_frequency(country_dfm)

textstat_frequency(region_dfm,groups="section")
textstat_frequency(region_dfm,groups="section") %>% ggplot(aes(y=docfreq,color=group,x=feature))+geom_point()
kwic_map <- kwic(epsa_tokens, newsmap_dict['AFRICA'])
kwic_map

dfm_group(region_dfm,"section") %>% dfm_weight("prop") %>% textstat_frequency(group="section",force=T) %>% ggplot(aes(y=frequency,color=group,x=feature))+geom_point()
```



### Applying dictionaries to different objects

Remember that we talked about the difference between corpus, tokens and dfms? Their structure is important for applying dictionaries.

Given words use their order in dfms, multi-word dictionaries have to be applied to the tokens.



```{r apply-dict2, exercise=T}

```

```{r apply3, exercise=T}

```

### To stem or not to stem?

Generally, I would recommend to skip stemming when using a dictionary: Since stemming cuts off the endings of words, we may include words we are not interested in or miss words that we actually wanted to include.

Instead, you can use **wildcards** to include words with different stems. 

Below, you can practice different versions of wildcards.

```{r wildcards,exercise=T}

```


## Analysing the results of the dictionary

```{r analyse-results, exercise=T}

```

```{r analyse2, exercise=T}

```




## Creating our own dictionary

To create our own dictionary, we need to come up with a list of words that represent the concepts we want to measure.

### Defining a dictionary

This is a list of the sections at this year's EPSA

- **EL**: Elections, Public Opinion, and Voting Behaviour (John Garry, QUB)
- **CP**: Comparative Politics (Ken Benoit, LSE)
- **LE**: Legislative Studies and Party Politics (Gail McElroy, TCD)
- **PE**: Political Economy (Tim Hicks, UCL)
- **EU**: EU Politics (Gail McElroy, TCD)
- **CF**: International and Domestic Conflict (Thomas Chadefaux, TCD)
- **IR**: International Relations (Jonathan Kuyper, QUB)
- **ME**: Political Methodology (Laron Williams, U. Missouri)
- **PA**: Public Policy and Public Administration (Eoin Oâ€™Malley, DCU)
- **PT**: Political Theory (Peter Stone, TCD)
- **PC**: Political Communication and Media (Rebekah Tromble, Leiden)
- (a single roundtbale submission that we have excluded)

Try to come up with a dictionary of words that could describe the content of each section. Check the documentation of the `dictionary()` function for more information on the syntax or click on 'hint' to see a skeleton of the syntax that you can fill in. If you click on solution, you will see a simple variant of a dictionary.

```{r dictionary,exercise=TRUE,exercise.setup="dict-setup",lines=15}
?dictionary
```

```{r dictionary-hint}
section_dictionary <- dictionary(list(
    EL=c(),
    CP=c(),
    LE=c(),
    PE=c(),
    EU=c(),
    CF=c(),
    IR=c(),
    ME=c(),
    PA=c(),
    PT=c(),
    PC=c()))
```

```{r dictionary-solution}
section_dictionary <- dictionary(list(
    EL=c("election*","opinion","vot*"),
    CP=c("comparative"),
    LE=c("parliament*","party*","parties*"),
    PE=c("econom*"),
    EU=c("EU","Europ*"),
    CF=c("conflict*"),
    IR=c("international*"),
    ME=c("method*"),
    PA=c("policy*"),
    PT=c("theor*"),
    PC=c("communication","media")))
```

To apply the dictionary, use the `tokens_lookup()` function (if you have used multi-word expressions - otherwise you can also go for `dfm_lookup`). Convert the result to a dfm and inspect it using `head()`.

```{r}
sections<-tokens_lookup(epsa_tokens,section_dictionary) %>% dfm()
head(sections)


```



## Using a dictionary for classification



We can use our dictionary to classify the sections. To make it easier to handle the results, we convert it to a dataframe using the `convert()` command. Additionally, we need to attach the document variables (which include the correct classification) to the data frame. Since the order has not changed, you can just use `bind_cols()` after converting the dfm to a data frame.

Next, we need to come up with a decision-making rule how to translate our dictionary hits into categories. While machine learning algorithms would come up with more elaborate rules, we use a very simple rule: We ascribe the abstract to the dictionary category that produces most hits in the text. This is a bit tricky and there are multiple ways to do this - feel free to take a look at the solution if you cannot figure out how to do this.



```{r classify,exercise=T}

```

```{r classify-solution,exercise=T}
section_classification<-convert(sections,"data.frame")
section_classification$predicted_section<-colnames(section_classification[,2:12])[max.col(section_classification[,2:12])]
```


Like other classification algorithms, we have to evaluate our dictionary against the gold standard (or is it a gold standard?) of the EPSA section distribution.

To make our life easier, we use the `confusionMatrix()` function from the caret package. This way, we just have to do three steps:

- convert the true and the predicted section into a factor - that is the input that `confusionMatrix()` takes
- apply the `confusionMatrix()` function
- interpret the results - we'll go through that in a second after you give the first two steps a try:


```{r evaluation,exercise=T}
caret::confusionMatrix(data=as.factor(toupper(section_classification$predicted_section)),reference=as.factor(section_classification$section))
```

First, have a look at the overall statistics. The most important statistic is **accuracy**. If you were as uncreative as me, your result will not be that great (e.g. somewhere between 30-40% accuracy). You also get a Confidence Interval - this is more important when you sample than in our case where we evaluate against the whole universe of cases. Even if your result is not that great, you are probably better than the **No-information Rate** which would be the result of random guessing.

If you want to go into details, you can check the statistics by class. **Sensitivity** shows how good your dictionary is at detecting positive instances of this section. In contrast, **Specificity** measures whether you have false positives, that is, results that you assign to the category while they really belong somewhere else. For each category, you also receive a 'Balanced Accuracy' which combines Sensitivity and Specificity.

These statistics by class are particularly useful when you want to find out where your classifier is still performing is performing best.
