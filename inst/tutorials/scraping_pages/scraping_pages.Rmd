---
title: "Scraping multiple pages"
output: 
  learnr::tutorial:
    progressive: false
runtime: shiny_prerendered
---

```{r setup, include=FALSE}
library(learnr)
library(learn2scrape)
library(rvest)
knitr::opts_chunk$set(echo = FALSE)
quotepage<-system.file("extdata", "quotepage.html", package = "learn2scrape") %>% read_html()
```



## Introduction

*Last revised: `r Sys.Date()`*

So, you have learned how to use some of the basic functions of the `rvest` package: `read_html()`, `html_nodes()` and `html_text()`.
But most of the time, we are not just interested in a single page but multiple pages from the same domain, e.g. all newspaper reports by a specific newspaper or all speeches by a politician. So we need another step: We have to learn to follow links without actually opening the browser and clicking on the link and copying the new path.

In this part of the tutorial, you will learn two things:

- to extract links from webpages
- ways to automate following these links.

You will see that in the end, webscraping is a function of programming in R. The more you learn to use loops, functions and apply commands, the easier the scraping will be. In the end, scraping is just a small step in the whole process of getting data.

## Extracting links from webpages

To extract links, we need another command. Remember we said hyperlinks are an *attribute* of the text? Because of that, the `rvest` command to get these links is called `html_attr()`. We can use it to extract different types of attributes, so you will have to tell rvest the attribute that we are interested in is a link. Remember what links looked like?

`This is text <a href="http://quotes.toscrape.com/">with a link</a>.`

*href* stands for hyperreference and signifies the webpage the link leads to. You can specify `name="href"` inside the `html_attr()` command to extract the link. For example:

`html_attr(parsed_page,"href")`

However, this will only work on individual HTML tags not on entire pages (since the link is an attribute of the tag), so we will use `html_nodes()` again. Please try two things on the [quotes2scrape webpage](http://quotes.toscrape.com/){target="_blank"}:

- first, you can extract potential links from all tags by using the universal selector (`html_nodes("*"")`)
- second, look up the selector of the tags again and extract the links to all tags

The parsed page is stored in an object called `quotepage` so you do not need to download it again.

```{r links,exercise=TRUE, exercise.lines=10}




```

```{r links-solution}
# all pages
quotepage %>% html_nodes("*") %>% html_attr("href")
# tags
quotepage %>% html_nodes(".tags .tag") %>% html_attr("href")
```


Do you notice something about the links? They are missing a part. That is because they are relative links within the directory structure of the webpage. To 'repair' them, we need to add the **base url** of the webpage. This is typically just the url of the webpage we originally scraped from.

For adding the base url, we can use the function `paste()` that pastes together two character vectors. I recommend using `paste0()` which pastes the vectors together without inserting separators like white space between the vectors. If you have never used paste, try it out:

```{r pastetest,exercise=T, exercise.lines=10}
paste("a","b")
paste0("a","b")
```

Now, completing the paths of the URLs we scraped should not be a problem for you. Re-use the code you used to extract the links of the tags, assign it to an object called `url` and add the base url (http://quotes.toscrape.com/) in front of it.

*Watch out for the slashes between the base url and the address of your page - having none or too many slashes is a typical problem!*

```{r paste, exercise=TRUE, exercise.lines=10}


```

```{r paste-solution}
urls<-quotepage %>% html_nodes(".tags .tag") %>% html_attr("href")
urls<-paste0("http://quotes.toscrape.com",urls)
urls
```



## Automating the following of links

As I mentioned, in the end, webscraping is a function of programming. So once we collected the links we are interested in and learned the basic commands of rvest, there are multiple ways to proceed:

- you can use a `for()`-loop that loops over the vector of links and scrapes each of them
- You can write a function that scrapes the content of all the links
    - you can put the function into your loop
    - you can `apply()` the function to a vector - this is the fastest variant but takes some getting used to

For now, we will start with the easiest variant and just create a `for`-loop. Later, we will also use `apply()` but there are good reasons why you will often return to simple loops.

I recommend to first write down a few lines of code as if you would just want to scrape the first link:

- extract the links to the pages for each quote
- parse the first of the quote pages
- extract the nodes of the quotes on this page
- extract the text of the first of these nodes using `html_node()` (for now, we extract just the first to make our life a bit easier)

This is also a good exercise to see to which extent you remember what we have learned so far. You can then think about re-writing the code in the next step.

```{r tagpages,exercise=TRUE, exercise.lines=20}





```

```{r tagpages-solution}
urls<-quotepage %>% html_nodes(".tags .tag") %>% html_attr("href")
urls<-paste0("http://quotes.toscrape.com",urls)
page<-read_html(urls[1])
selected_nodes<-html_node(page,".text")
pagetext<-html_text(selected_nodes)
```

Now, we to analyze the code: which part will vary when you try to repeat this multiple times?

```{r selector-practice, echo=FALSE}
quiz(caption= "How to repeat this",
question("Which parts of the code change when we want to scrape several pages?",
  answer("the URL", correct = TRUE),
  answer("the selector",message="Selectors sometimes change but not necessarily!"),
  answer("the URL and the selector",message="Selectors sometimes change but not necessarily!"),
  answer("we need to repeat the code for each iteration",message="don't be such a pessimist!"),
  allow_retry=TRUE
))
```

If you have figured this out, it is time to try it!

## Using `for`-loops

Now, try to write the code into a loop. Remember how `for`-loops work? We need to find a way to *loop* over our list of urls, parsing one after the other and then reading out the quotes. If you don't know how to do this, you can check the hint!

Also, to keep our results, make sure to create an empty object in which you will store the text of the quotes! Again, you can check the hint if you do not know how to do this.


```{r loop,exercise=TRUE, exercise.lines=20}

```


```{r loop-hint}
urls<-quotepage %>% html_nodes(".tags .tag") %>% html_attr("href")
urls<-paste0("http://quotes.toscrape.com",urls)
pagetext <- character(0)
for(i in 1:length(urls)){
  page <- read_html(urls[i])
# actual scraping code


}
pagetext
```
```{r loop-solution}
urls<-quotepage %>% html_nodes(".tags .tag") %>% html_attr("href")
urls<-paste0("http://quotes.toscrape.com",urls)
pagetext <- character(0)
for(i in 1:length(urls)){
  page <- read_html(urls[i])
  pagetext[i] <- page %>%
    html_node(".text") %>%
    html_text()
}
pagetext
```



## Wrap-up

Fantastic, you're done with this lesson! We will repeat to similar tasks in the next days, also using `apply()` and other ways of looping. Still, `for`-loops are super practical for many simple scraping tasks!

The more you learn to use loops, functions and apply commands, the easier the scraping will be. In the end, scraping is just a small step in the whole process of getting data so if you improve your programming skills in R - which is rewarding anyway - you will also get better at scraping in R.
