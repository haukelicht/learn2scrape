---
title: "Download data using Twitter's Streaming API"
# author: "Theresa Gessler and Hauke Licht"
# date: "last updated: `r format(Sys.Date(), '%e %B %Y')`"
description: >
  Learn how to download a random sample of Tweets absed on keywords and other filter criteria 
  using the Twitter Streaming API 
  with the `rtweet` R package.
output: 
  learnr::tutorial:
    progressive: true
runtime: shiny_prerendered
---

```{r knitr, include=FALSE}
knitr::opts_chunk$set(
  # code chunk options
  echo = TRUE
  , eval = TRUE
  , warning = FALSE
  , message = FALSE
  , cached = FALSE 
  , exercise = TRUE
  , exercise.eval = FALSE
  , fig.align = "center"
  , fig.height = 4
  , fig.width = 5.5
)
```


```{r setup-invisible, exercise = FALSE, include = FALSE}
library(learnr)
library(learn2scrape)
```

## Introduction

In this tutorial, you will learn how to download data from the *Twitter Streaming* API using the `rtweet` package.

We will use the following R packages:

```{r setup, exercise = FALSE}
library(rtweet)
library(jsonlite)
library(dplyr)
library(tidyr)
library(stringr)
```


## Authenticating

Before we can start collecting Twitter data, we need to create an OAuth token that will allow us to authenticate our connection and access our personal data.

Obtaining valid API credentials requires submitting an application for a developer account at https://developer.twitter.com/en/apps
This may take a few days. 
For teaching purposes only, I will temporarily share the credentials of one of my Apps, so that we can use the API without having to do the authentication.
Below, I load these credentials and create an rtweet 'token' object from them.
(Note: `do.call()` just calls `create_token()` while passing the name-value pairs specified in the list object 'credentials')

```{r authenticate, exercise = FALSE}
# specify path to credientials JSON
creds_fp <- system.file("extdata", "tw_credentials.json", package = "learn2scrape")

# read credentials from packaged JSON file
credentials <- fromJSON(creds_fp)

# create rtweet 'token' object
token <- do.call(create_token, credentials)
```

<br/>
<details>
<summary>**How to obtain Twitter API credentials**</summary>

Follow these steps to create your token.

1. Go to https://developer.twitter.com/en/apps and sign in with your Twitter account.  
2. If you don't have a developer account, you will need to apply for one first. Fill in the application form and wait for a response.
3. Once it's approved, click on "Create New App". You will need to have a phone number associated with your account in order to be able to create a token.  
4. Fill name, description, and website (it can be anything, even http://www.google.com). Make sure you leave 'Callback URL' empty.
5. Agree to user conditions.  
6. create an empty JSON file with the following structure

```json
{
  "app": "",
  "consumer_key": "",
  "consumer_secret": "",
  "access_token": "",
  "access_secret": ""
}
```

6. From the "Keys and Access Tokens" tab, copy-paste the consumer key and consumer secret information into the JSON file
7. Click on "Create my access token", then copy--paste your access token and access token secret into the JSON file
8. save the JSON file somewhere secure.

With the following R code, you can then always read the API credentials and create an rtweet 'token' object: 

```{r , exercise = FALSE, eval = FALSE}
library(jsonlite)
library(rtweet)
credentials <- fromJSON(system.file("extdata", "tw_credentials.json", package = "learn2scrape"))
token <- do.call(create_token, credentials)
```

If you are interested in a more detailed explanation, check `vignette("auth", package = "rtweet")`

To check that it worked, try running the line below:

```{r lookup-theresa}
lookup_users("th_ges")[["name"]]
```

**What can go wrong here?**
Make sure all the consumer and token keys are pasted here as is, without any additional space character. 
If you don't see any output in the console after running the code above, that's a good sign.
</details>

## Collecting tweets

To collect tweets as they are sent out, we can use the `stream_tweets()` function.
By default, `stream_tweets()` downloads a *random sample* of all publicly available tweets.

It has the following paraemters:

- `file_name` indicates the file (path) where the tweets will be downloaded to on your local system
<!-- - `dir` (optional) indicated the file path where to write the downloaded data to -->
- `timeout` is the number of seconds that the connection will remain open. If you set it to `FALSE`, it will stream indefinitely until the rate limit is reached.
- `parse` specifies whether the tweets should be parsed from JSON. By default, it is `TRUE`. But if you try to collect more data, your script will run better if you disable this and set `parse = FALSE` because we omit the JSON parsing step before writing to `file_name`. 

Now, we can collect tweets for 10 seconds. 

```{r stream-tweets-default}
# determine path where to write JSON to
fp <- file.path(dirname(getwd(), "tweets.json"))

resp <- stream_tweets(file_name = fp, timeout = 10, parse = FALSE)
```

**_Note:**: We write the JSON to a file in the directory *above* the current working directory, because in `learnr` tutorials, each code chunk has its own temporary directory that is deleted (incl. its contents) after execution.

Once the time out has been reached, we can open the file in R as a data frame with the `parse_stream` function:

```{r parse-stream-tweets}
fp <- file.path(dirname(getwd(), "tweets.json"))
tweets <- parse_stream(fp)

# inspect
nrow(tweets) # number of downloaded tweets
range(tweets$created_at) # time range of downloaded tweets
```


## Filtering Tweets

There are multiple variants how we can use the streaming API:

1. Sampling a small random sample of all publicly available tweets --- that is what we did above!

2. Filtering via a search-like query (up to 400 keywords)

3. Tracking via vector of user ids (up to 5000 user IDs)

4. Location via geo coordinates

### Filtering by keyword

To filter by keyword, we have to specify our search term as query `q`:

```{r stream-tweets-by-keyword, eval = FALSE}
stream_tweets(q = "news", timeout = 10, file_name = "news.json")
```

### Filtering by users

We could also provide a list of users (user IDs or screen names).
However, this makes much more sense when looking at timelines and searching for previous tweets.
We will do this in the next exercise.

### Filtering by geo location

This second example shows how to collect tweets filtering by geo location instead. 
In other words, we can set a geographical box and collect only the tweets that are coming from that area.
After that, we can again load the tweets from disk into R:

For example, imagine we want to collect tweets from the United States. 
The way to do it is to find two pairs of coordinates (longitude and latitude) that indicate the southwest corner AND the northeast corner. 
Note the reverse order: it's not (lat, long), but (long, lat)!

In the case of the US, it would be approx. (-125, 26) and (-65, 49). 
How to find these coordinates? 
I use: `https://getlatlong.net/` 
If you have a *Google Maps* API key, you can also use the `lookup_coords()` function built into `rtweet`.

```{r stream-tweets-by-geo, eval = FALSE}
fp <- file.path(dirname(getwd()), "tweets_usa.json")
usa <- stream_tweets(
  q = c(-125, 26, -65, 49),
  timeout = 10,
  file = fp
)

usa_tweets <- parse_stream(fp)
```

And use the maps library to see where most tweets are coming from. 
Note that there are different types of geographic information on tweets, some of it comes from geo-located tweets and others from tweets with place information.
`rtweet` has a function called `lat_lng()` that uses whatever geographic information is available to construct latitude and longitude variables. 
We will work with whatever is available.

```{r parse-geo-tweets}
fp <- file.path(dirname(getwd()), "tweets_usa.json")
usa_tweets <- parse_stream(fp)
usa_tweets <- lat_lng(usa_tweets)

# plot lat and lng points onto state map
maps::map("state", lwd = .25)
with(usa_tweets, points(lng, lat, pch = 20, cex = .75, col = rgb(0, .3, .7, .75)))
```


## Some analyses

We now do some basic text analysis. 
This is not the focus of this class and you might want to do this differently, depending on which package you usually work with.
<!-- Still, it is no fun to not see our results! -->

For example, we can ask what are the *most popular hashtags* at the moment? 
We will use [regular expressions](https://en.wikipedia.org/wiki/Regular_expression) to extract hashtags.

The function `str_extract_all()` in the `stringr` pacakge extracts one or several matches from a character vector. 
The expression "#\\w+" is a *regular expression*. 
Specifically, it matches a hashtag-symbol and then any number of uninterrupted alpha-numeric symbols - so numbers or (upper- or lowercase) latin letters, and underscore. 
Since `str_extract_all()` returns a list of character vectors (one list element per input character value), we have to unlist the return object.
Finally, to get at the $k$ most popular hashtags, we sort the resulting vector by decreasing frequency.

```{r extract-hashtags, eval = FALSE}
# load stream of tweets downloaded before
fp <- file.path(dirname(getwd(), "tweets.json"))
tweets <- parse_stream(fp)

# extract hashtags
ht <- str_extract_all(tweets$text, "#\\w+")
ht <- unlist(ht)

# tabulate 6 most frequent ones
head(sort(table(ht), decreasing = TRUE))
```

<br/>

Similar analyses could be implemented for the following questions:
<details>
<summary>The most frequently mentioned users?</summary>

We again use a regular expression and `str_extract_all()`. 
Our search string is similar but it starts with an @ - so we find mentions - and this time, we only include Latin characters, numbers, and underscores.

```{r extract-mentions, eval = FALSE}
# load stream of tweets downloaded before
fp <- file.path(dirname(getwd(), "tweets.json"))
tweets <- parse_stream(fp)

# extract mentions
mentions <- str_extract_all(tweets$text, '@[0-9_A-Za-z]+')
mentions <- unlist(mentions)

# report 10 most frequently mentioned accounts
head(sort(table(mentions), decreasing = TRUE), n = 10)
```
</details>

<details>
<summary>How many tweets mention Donald Trump?</summary> 

We try to *detect* tweets that mention either 'Trump' or 'trump' using `str_detect()` and sum them up.

```{r count-Trump, eval = FALSE}
# load stream of tweets downloaded before
fp <- file.path(dirname(getwd(), "tweets.json"))
tweets <- parse_stream(fp)

# count number of times terms 'trump'/'Trump' occur
sum(str_detect(tweets$text, "[Tt]rump"))
```
</details>

<br/> 

These are toy examples, but for large files with tweets in JSON format, there might be faster ways to parse the data. 
For example, the `jsonlite` package specializes on parsing json data.
