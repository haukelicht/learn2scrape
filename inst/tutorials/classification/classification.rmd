---
title: "Text Classification Tutorial"
output: learnr::tutorial
runtime: shiny_prerendered
---

```{r setup, include=FALSE}
library(learnr)
library(dplyr)
library(learn2scrape)
library(caret)
library(quanteda)
knitr::opts_chunk$set(echo = FALSE,
exercise.timelimit=500)

data(epsa_paper)
epsa_paper<-subset(epsa_paper,section!="PS")
epsa_corpus<-corpus(epsa_paper,text_field = "abstracts")
epsa_tokens<-tokens(epsa_corpus,remove_hyphens=T,remove_symbol=T,remove_punct=T)
epsa_dfm<-dfm(epsa_tokens) %>% dfm_remove(c(stopwords("en"),"-"))
epsa_dfm_unstemmed<-epsa_dfm
epsa_dfm<-dfm_wordstem(epsa_dfm)
docvars(epsa_dfm,"section")<-as.factor(docvars(epsa_dfm,"section"))
data(iris)
training_sample<-dfm_sample(epsa_dfm,size=700)
test_sample<-dfm_subset(epsa_dfm,!docnames(epsa_dfm) %in% docnames(training_sample))
training_sample<-dfm_trim(training_sample,1)
nb_model<-textmodel_nb(x=training_sample,y=docvars(training_sample,"section"))
```

## Introduction

*Last revised: `r Sys.Date()`*

This tutorial introduces supervised text classification with machine learning. I have preloaded the `caret`, `dplyr` and `quanteda` packages. I have also pre-processed the EPSA data into a document-feature matrix called `epsa_dfm`. Just for your information: I used stemming and removed stopwords, hyphens and symbols.


## Basics

We have discussed that we need to split the sample into a training and a test set. Here, we will just use sampling function built into `quanteda`. However, we will discuss how to sample in other packages (and how to sample non-text data) at the end of this tutorial.

### Sampling in quanteda

quanteda comes with a `dfm_sample()` function. With this, you can easily create a random training sample - you just need to decide on the size. In our case, I suggest to put most data into the training sample as we anyway do not have so much information to learn from. If you do not know the syntax, have a look at the documentation on [value matching](https://stat.ethz.ch/R-manual/R-devel/library/base/html/match.html).

```{r quanteda-split,exercise=TRUE}

```


```{r quanteda-split-solution, eval=F}
training_sample<-dfm_sample(epsa_dfm,size=700)
test_sample<-dfm_subset(epsa_dfm,!docnames(epsa_dfm) %in% docnames(training_sample))
```

### Aligning training and test set

Next, you should make sure that you only use features on which you actually have information - words that do not occur in your training set will not be useful for classifying. So you can use the `dfm_trim()` function to keep only words that occur in your sample - just specify 1 as the threshold.

Then, you can also restrict the features of the test set to those features mentioned in the training set - otherwise, your classifier will not work. For this, you can use `dfm_keep()` (or `dfm_select()` with specifying that selection="keep" - the first variant is just a short-hand).


```{r quanteda-features,exercise=TRUE,exercise.lines=4}

```

```{r quanteda-features-solution}
training_sample<-dfm_sample(epsa_dfm,size=700)
test_sample<-dfm_subset(epsa_dfm,!docnames(epsa_dfm) %in% docnames(training_sample))
training_sample<-dfm_trim(training_sample,1)
test_sample<-dfm_keep(test_sample,training_sample)
```

Keep in mind the difference between `dfm_trim()` which cuts features based on their frequency and `dfm_keep` which changes features based on their presence in a character vector or another dfm.


## Training a classifier

Now, we get to the interesting stuff.
So far, `quanteda` is relatively basic regarding classifiers: The CRAN-version (which you have probably installed) only includes `textmodel_nb()`, a Naive Bayes classifier. However, naive bayes performs quite well for many tasks. Additionally, the quanteda implementation [is optimized for text and uses a sparse matrix](https://stackoverflow.com/questions/54427001/naive-bayes-in-quanteda-vs-caret-wildly-different-results) which makes it much faster (and sometimes better performing) than Naives Bayes classifiers included in other packages when working with text.

We try to do everything in one step, so take your time:

- recreate the training sample and the test set
- trim and align them
- train the classifier using `textmodel_nb()`
- predict the sections of the abstracts using the `predict()` command
- create the `confusionMatrix` we introduced in the section on using dictionaries for classification


```{r quanteda-classify,exercise=TRUE,exercise.lines=10}

```

```{r quanteda-classify-solution,eval=F}
training_sample<-dfm_sample(epsa_dfm,size=700)
test_sample<-dfm_subset(epsa_dfm,!docnames(epsa_dfm) %in% docnames(training_sample))
training_sample<-dfm_trim(training_sample,1)
test_sample<-dfm_keep(test_sample,training_sample)

classifier<-textmodel_nb(x=training_sample,y=docvars(training_sample,"section"))
predict<-predict(classifier,newdata=test_sample)
confusionMatrix(as.factor(docvars(test_sample,"section")),predict)
```

Currently, more classifiers specifically for quanteda are being developed in the `quanteda.classifiers` package which you can already install from [github](https://github.com/quanteda/quanteda.classifiers). So watch this in the future!






## Classifier speed

When we run classifiers, this can sometimes become really lengthy. I try to keep a record of the duration by assigning `Sys.time()` to a variable before and after I start the process. Calculating the difference then tells me how long this took. Try it out!

```{r twoplustwo, exercise=TRUE}
start<-Sys.time()
2+2
end<-Sys.time()
end-start
```

We can also use this to evaluate how long a classifier takes. Try to check the time of the classifier we wrote in the previous section. You just need to add a few lines of code in the right places. Of course, there are multiple places where you could want to measure the time - so it is not necessarily wrong if you add the timers at a different place than the solution*:

```{r timeone,exercise=TRUE,exercise.lines=15}
training_sample<-dfm_sample(epsa_dfm,size=700)
test_sample<-dfm_subset(epsa_dfm,!docnames(epsa_dfm) %in% docnames(training_sample))
training_sample<-dfm_trim(training_sample,1)
classifier<-textmodel_nb(x=training_sample,y=docvars(training_sample,"section"))
test_sample<-dfm_keep(training_sample)
predict<-predict(classifier,newdata=test_sample)
```

```{r timeone-solution}
training_sample<-dfm_sample(epsa_dfm,size=700)
test_sample<-dfm_subset(epsa_dfm,!docnames(epsa_dfm) %in% docnames(training_sample))
training_sample<-dfm_trim(training_sample,1)
start<-Sys.time()
classifier<-textmodel_nb(x=training_sample,y=docvars(training_sample,"section"))
end<-Sys.time()
test_sample<-dfm_keep(training_sample)
predict<-predict(classifier,newdata=test_sample)
end-start
```

## Evaluating feature thresholds

We talked about the importance of removing features. This exercise is supposed to be a playground for you to evaluate how different thresholds affect the *speed*, *number of features* and *accuracy* of our classifier. To do this at scale, we should create a single test- and training set and then write a function that uses different thresholds for trimming and evaluates the respective classifiers. If you feel confident, try to write the function yourself. If you feel like you still need some help, copy the function from the first hint and just add a `sapply()`-command with different thresholds for `dfm_trim` to see how the classifier speed and accuracy develops.

If you decide to write the function yourself, feel free to re-use the code from above. Just remember that you will need to trim the features of the test sample depending on the threshold you use for the training sample, so you might need to restructure things a bit.

However, be a bit cautious with the interpretation of your results: ultimately, every run is a bit random and accuracy will vary depending on the random sample - so look at the confidence intervals. *If you run the classifiers a few times, you will see that the results vary a lot. This is because in the end, we have a quite small training and test set!* 
Also, the results for spped might be very different for other data sets and classifiers so take your time to practice with other data as well if you have the opportunity.


```{r timetwo, exercise=TRUE,exercise.lines=20}



```
```{r timetwo-hint}
epsa_dfm<-dfm_wordstem(epsa_dfm) 
training_sample<-dfm_sample(epsa_dfm,size=700)
test_sample<-dfm_subset(epsa_dfm,!docnames(epsa_dfm) %in% docnames(training_sample))


test_performance<-function(n){
    training_sample<-dfm_trim(training_sample,n)
    start<-Sys.time()
    classifier<-textmodel_nb(x=training_sample,y=docvars(training_sample,"section"))
    end<-Sys.time()
    test_sample<-dfm_keep(test_sample,training_sample)
    predict<-predict(classifier,newdata=test_sample)
    confmat<-confusionMatrix(data=predict,reference=as.factor(docvars(test_sample,"section")))
    performance=list(time=end-start,
                     features=length(featnames(training_sample)),
                     accuracy=confmat$overall[1],
                     accuracy_lower=confmat$overall[3],
                     accuracy_upper=confmat$overall[4])
    return(performance)
}
```
```{r timetwo-solution}
epsa_dfm<-dfm_wordstem(epsa_dfm) 
training_sample<-dfm_sample(epsa_dfm,size=700)
test_sample<-dfm_subset(epsa_dfm,!docnames(epsa_dfm) %in% docnames(training_sample))

test_performance<-function(n){
    training_sample<-dfm_trim(training_sample,n)
    start<-Sys.time()
    classifier<-textmodel_nb(x=training_sample,y=docvars(training_sample,"section"))
    end<-Sys.time()
    test_sample<-dfm_keep(test_sample,training_sample)
    predict<-predict(classifier,newdata=test_sample)
    confmat<-confusionMatrix(data=predict,reference=as.factor(docvars(test_sample,"section")))
    performance=list(time=end-start,
                     features=length(featnames(training_sample)),
                     accuracy=confmat$overall[1],
                     accuracy_lower=confmat$overall[3],
                     accuracy_upper=confmat$overall[4])
    return(performance)
}
sapply(c(1,5,10,20,50,100),test_performance)
```


## Substantive feature evaluation

While many machine learning algorithms are a black box, with Naives Bayes, it is relatively easy to see which features are most important in the determination of each class. You can use `coef()` to see the coefficients of each feature.

To see which features are most meaningful for different classes, convert the coefficient matrix into a data frame that you can sort by the feature frequency. The feature names will be stored as row names but you can convert them into a variable.

Have a look at the 20 features which are most associated with the conflict section (CF)! *If you do not want to copy the model code again, I have pre-estimated a classifier called* `nb_model`. 

```{r featureeval, exercise=TRUE}

```
```{r featureeval-solution}
coef(nb_model)
evaluation_df<-coef(nb_model) %>% data.frame() %>% mutate(feature=rownames(.))
evaluation_df %>%  arrange(desc(CF)) %>% select(feature, CF) %>% head(20)
```

Are the features similar to what you thought of for the dictionary?

## Additional models

If you think you would like to try more classifiers, `caret` is your friend. Check the [list of models by tag](https://topepo.github.io/caret/train-models-by-tag.html) to look through the models caret has implemented. Just be aware that many of them take quite long since they are not optimized for text classification. Also, some have specific restrictions (e.g. they can only distinguish between two classes). Below, we just discuss the most important differences if you want to try your luck with caret.

There are other packages built for text classification like `RTextTools`, however, I do not recommend them since they are not maintained anymore and there is no support in case of problems. If you have previously used `tidytext()`, I am also a fan of [this tutorial](https://juliasilge.com/blog/tidy-text-classification/).


### Sampling in caret

Sampling in `caret` is a bit more tedious - we will not go through it in detail, there are just two details I would like to point out:

1. With any other package but quanteda, we have to convert the dfm into a data frame to do that. You can do that using `convert(dfm-name,"data.frame")`, so that is easy enough. 
2. `caret` has a lot more options for sampling. So if you want to stratify your sample by groups (e.g. to over-sample an under-represented class to make sure your classifier learns it), it is worth getting acquainted with the function a bit more. If you are interested, just check the documentation below. Otherwise, it can be simpler to still do the sampling in quanteda so you won't have to deal with empty features later on.

```{r caret-docu,exercise=TRUE}
?createDataPartition
```

Empty features are less of a problem when you work with data other than text - typically, the variables from which our algorithm learns are already set. Then, separating the data is pretty straight-forward. Try it out on the `iris` dataset, a classic dataset used for machine learning:

```{r caret-sample, exercise=TRUE}

```

```{r caret-sample-solution}
training_id <- createDataPartition(iris$Species, p=0.80, list=FALSE)
validation <- iris[-training_id,]
training<-iris[training_id,]
```



### Applying a classifier in caret

The beauty of caret is that it unifies the syntax of lots of classification packages within R. The central function is `train()` and you can use it for different classifiers by just specifying a different method. We keep using the iris dataset so that our models actually compile

I included code below that you can expand if you want

- `trainControl()` is a function that splits the training sample into multiple parts for [cross-validation](https://machinelearningmastery.com/k-fold-cross-validation/) 
    - I just include it to show you the command if you have previously done machine learning with other packages but just ignore it if you are new to this
- we `train()` or *fit* different models to the data and calculate the accuracy based on re-sampling the training sample
    - this is a first measure of performance but it only tells us about performance on the data we used to train
- The three different models we calculate are:
    - lda
    - k nearest neighbours
    - a support vector machine
- we summarize the different models we ran, based on their within-sample performance on the training sample
- to actually evaluate our performnace, we look at the confusion matrix on the held-out test set

```{r caret-classify,exercise=TRUE}
control <- trainControl(method="cv", number=10)
metric <- "Accuracy"
fit.lda <- train(Species~., data=training, method="lda", metric=metric, trControl=control)
fit.knn <- train(Species~., data=training, method="knn", metric=metric, trControl=control)
fit.svm <- train(Species~., data=training, method="svmRadial", metric=metric, trControl=control)
    
# summarize accuracy of models
results <- resamples(list(lda=fit.lda, knn=fit.knn, svm=fit.svm))
summary(results)

# predict
predictions <- predict(fit.lda, validation)
confusionMatrix(predictions, validation$Species)
```



## Wrap-up

If you are done and still motivated to practice, there are a few more things I recommend to try:

- evaluate different training set sizes to get a feel for the differences
- reproduce the classification in your own R console and evaluate different pre-processing steps like stemming
- predicting something else, e.g.: how well does the abstract tell us the author gender?
    - for this, you can install the `gender` package which is quite good at telling us author genders by first names

