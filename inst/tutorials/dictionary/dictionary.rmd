---
pagetitle: "Creating a dictionary for classification"
output: 
  learnr::tutorial:
    progressive: false
runtime: shiny_prerendered
---

```{r setup, include=FALSE}
library(learnr)
knitr::opts_chunk$set(echo = FALSE,message=F,warning=F)
library(quanteda)
library(dplyr)
library(ggplot2)
library(learn2scrape)

data(epsa_paper)
epsa_paper<-subset(epsa_paper,section!="PS")
```

## Welcome

*Last revised: `r Sys.Date()`*

This is the tutorial for dictionary analysis. We will proceed step-by-step and also go through some of the most important steps in pre-processing data.

For our analysis, we first need to load some packages: **quanteda** (for text analysis), **dplyr** (for data manipulation) and **ggplot2** (for visualizing our results). I have already pre-loaded the data frame we will use. It is called **epsa_paper**.

```{r libraries,exercise=T}
library(quanteda)
library(dplyr)
library(ggplot2)
```

## Pre-processing data

### Pre-processing steps

As mentioned, there are several stages to the pre-processing process:

- creating a corpus: `corpus()`
- tokenizing the corpus: `tokens()`
- generating a document feature matrix: `dfm()`

You can try all three steps below. Just some notes of caution: `corpus` has great defaults and you can often just apply it to your data frame; however, in our case you need to tell the function where to find the text by specifying the text_field option inside the command. If your data frame would have a variable called text, the `corpus()` function would take that.

While the main purpose of `tokens()` is to split the text into individual words, it already allows you to do a lot of pre-processing. I recommend to specify remove_hyphens=T, remove_symbol=T and remove_punct=T to remove these things and split hyphenated words. After all, political scientists love using complex words in their abstracts.

The `dfm()` function automates a lot of pre-processing and I would recommend to check the documentation to see the default settings. For example, it automatically sets all words to lowercase. With `dfm_remove()` we can remove features we are not interested in, e.g. so-called [stopwords]() that occur frequently but are not meaningful (e.g. 'is'). Usually, we would also stem words at this point with `dfm_wordstem()` - this is not so crucial and sometimes counter-productive for dictionaries (but more on that below).

```{r corpus, exercise=T,exercise.setup='packset'}


```

```{r corpus-hint,eval=F}
?corpus
?tokens
?dfm
```

```{r corpus-solution}
epsa_corpus<-corpus(epsa_paper,text_field = "abstracts")
epsa_tokens<-tokens(epsa_corpus,remove_hyphens=T,remove_symbol=T,remove_punct=T)
epsa_dfm<-dfm(epsa_tokens)  %>% dfm_remove(stopwords("en"))
#normally
#epsa_dfm <- epsa_dfm %>% dfm_wordstem(language="en")
```

<div id="corpus-hint">`epsa_corpus<-corpus(epsa_paper,text_field = "abstracts")`<br>
`epsa_tokens<-tokens(epsa_corpus,remove_hyphens=T,remove_symbol=T,remove_punct=T)`<br>
`epsa_dfm<-dfm(epsa_tokens)  %>% dfm_remove(stopwords("en"))`<br>
`#normally: #epsa_dfm <- epsa_dfm %>% dfm_wordstem(language="en")`
</div>


## Loading an existing dictionary


```{r dictset,include=F}
epsa_corpus<-corpus(epsa_paper,text_field = "abstracts")
epsa_tokens<-tokens(epsa_corpus,remove_hyphens=T,remove_symbol=T)
epsa_dfm<-dfm(epsa_tokens) %>% dfm_remove(c(stopwords("en"),"-"))
```

Dictionaries come in different forms. For pre-defined dictionaries, YAML is one of of the most frequent formats. For our example, we will use the [newsmap dictionary](). Normally, you would just load it by giving the path to the file. Since this is a bit more complicated when you load the file from a package, I have already written a line that finds the correct path for you.
 First, try to load the dictionary using the `dictionary()` function. Check the documentation to see the syntax or ask for a hint.

```{r loaddict, exercise=T,exercise.setup='dictset',exercise.lines=3}
path<-system.file("extdata", "newsmap.yml", package = "learn2scrape")

```

```{r loaddict-hint}
path<-system.file("extdata", "newsmap.yml", package = "learn2scrape")
newsmap_dict <- dictionary(file = path,
                           format = "YAML")

```


To view the dictionary, you can use the `print()` command

```{r printdict-setup}
path<-system.file("extdata", "newsmap.yml", package = "learn2scrape")
newsmap_dict <- dictionary(file = path,
                           format = "YAML")
```

```{r printdict,exercise=T}
print(newsmap_dict)
```


## Applying the dictionary

```{r applysetup,include=F}
epsa_corpus<-corpus(epsa_paper,text_field = "abstracts")
epsa_tokens<-tokens(epsa_corpus,remove_hyphens=T,remove_symbol=T)
epsa_dfm<-dfm(epsa_tokens) %>% dfm_remove(c(stopwords("en"),"-"))
path<-system.file("extdata", "newsmap.yml", package = "learn2scrape")
newsmap_dict <- dictionary(file = path,
                           format = "YAML")
```


To apply the dictionary, we can use `tokens_lookup()` or `dfm_lookup()` on the dfm or corpus object. You can try it out below. Note that I have put a `head()` around the tokens variant, otherwise you will see the tokens of all 800 abstracts in our dataset. If you want to get a general overview of the results, you can also use the `textstat_frequency()`command to get a count frequency of all the features:

```{r simpleapply,exercise=T,exercise.setup="applysetup"}
dfm_lookup(epsa_dfm,newsmap_dict)
tokens_lookup(epsa_tokens,newsmap_dict) %>% head()
```


### Applying dictionaries to different objects

Remember that we talked about the difference between corpus, tokens and dfms? This is important for applying dictionaries. 
Given words loose their order in dfms, multi-word dictionaries have to be applied to tokens, not dfms. You can then convert the tokens object into a dfm in the next step. Try it out below! (That is: create a dfm from the text and apply the dictionary, create tokens from the text and apply the dictionary)

```{r objectapply,exercise=T}
text<-"Switzerland is not in the European Union"
eu_dict<-dictionary(list(EU="European Union"))


```

```{r objectapply-solution,eval=F}
text<-"Switzerland is not in the European Union"
eu_dict<-dictionary(list(EU="European Union"))
dfm(text) %>% dfm_lookup(eu_dict)
tokens(text) %>% tokens_lookup(eu_dict) %>% dfm()
```

### Apply different levels

Dictionaries with a hierarchical structure can be evaluated at different levels. The newsmap dictionary has such a hierarchical structure: It contains countries, nested in regions which are then nested in continents. That means, you can apply only a part of the dictionary by specifying `levels` inside the `dfm_lookup()` command. Try it out below by specifying levels (1, 2 or 3) and look at the output of `textstat_frequency()`!

```{r applylevels, exercise=T,exercise.setup='applysetup'}




```

```{r applylevels-solution,eval=F}
# apply one level of the dictionary (continent)
region_dfm <- dfm_lookup(epsa_dfm, newsmap_dict, levels = 1)
textstat_frequency(region_dfm)
country_dfm <- dfm_lookup(epsa_dfm, newsmap_dict, levels = 3)
textstat_frequency(country_dfm)
```


### Evaluation by subgroups

Remember all the information we originally had in the data frame? The quanteda objects keep this information as so-called *document variable* with each document, though it is hidden. You can access it for any corpus, tokens or dfm object with the `docvars()` command. You can also use it to group the output of the `textstat_frequency()` function. For example, we have information on the section of each paper. This was a bit messy in the dataset we scraped but I stored a cleaned version in the variable `section`. 

Let's apply this: You can see the original sections below. I would recommend you only apply the highest level of the dictionary (levels=1) to keep the output manageable. Then, tell `textstat_frequency` to sort the output by specifying the `section` variable as a group.

```{r groupapply, exercise=T,exercise.setup='applysetup'}


```

```{r groupapply-solution,eval=F}
dfm_lookup(epsa_dfm, newsmap_dict, levels = 1) %>%
  textstat_frequency(groups="section")
```


<!--Bonus: If you want to visualize this, you can use `dfm_weight()` to calculate proportions by group.-->

```{r,eval=F,include=F}
dfm_lookup(epsa_dfm, newsmap_dict, levels = 1) %>% dfm_group("section") %>% dfm_weight("prop") %>% textstat_frequency(group="section") %>% ggplot(aes(y=frequency,color=group,x=feature))+geom_point()
```


Of course, there is a lot more to learn about dictionaries - for example, you can combine them with [weighting]() and receive quantities like the relative frequency of your dictionary entries or dichotomize the results into match / no match. This all depends on your research question, so please explore and feel free to ask. We now proceed to creating our own dictionary in order to practice classifying texts.
