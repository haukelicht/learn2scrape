---
title: "Downloading files"
output: learnr::tutorial
runtime: shiny_prerendered
---


```{r setup, include=FALSE}
library(learnr)
library(rvest)
knitr::opts_chunk$set(echo = FALSE)
```

### Introduction

Similar to scraping tables, downloading files is pretty easy. Actually, it is one of the tasks for which we do not even need `rvest`!

Essentially, downloading files works with base-R and all that our knowledge of scraping does is make it easier for us to collect the links and to think through how to download files

### Downloading Files in R

The central command for us is `download.file()`. If you have never used it, check [its documentation](https://stat.ethz.ch/R-manual/R-devel/library/utils/html/download.file.html){target="_blank"}.

Essentially, you need to specify the **url** you are downloading from, the **destfile** (destination file: where and under which name your file will be saved) and the **method** of downloading. The method of downloading works a bit different on each operating system, so I would again recommend that you check the [documentation](https://stat.ethz.ch/R-manual/R-devel/library/utils/html/download.file.html){target="_blank"} if the default does not work for you. The same holds for the **mode** of downloading.

To be honest, situations in which you have to download files rather than HTML pages are scarce, still. Let's pretend we want to download the [Congressional Record of the ongoing session](https://www.congress.gov/congressional-record/116th-congress/browse-by-date){target="_blank"} in its [beautiful original layout as a pdf](https://www.congress.gov/116/crec/2019/06/28/CREC-2019-06-28.pdf){target="_blank"}. There are several steps to this:

- find the appropriate selectors and scrape all links to pdf from the [page of the Congressional Record of the ongoing session](https://www.congress.gov/congressional-record/116th-congress/browse-by-date){target="_blank"}
- create destination filenames, using the command `basename()` - this keeps only the last part of the path of the pdf. If you don't specify something different, the files will go to your working directory
- use `mapply()` to apply the `download.file()` function to both the file name and the destination file; for me the correct mode is "wb" but something different might work for you
    - if you use `sapply()` or a similar function, only the url will vary but the document will always be saved under the same name - `mapply()` can go over multiple varying vectors (in our case: URLs and filenames)
    - it works like this: `mapply(function, vector1, vector2, otherarguments)` - note that this is a different order than for `sapply(vector, function,otherarguments)`
    - if it is easier for you, you can also write a function that wraps `download.file()` with the correct mode

*Pro-Tip: Don't download all files since we won't use them - just cut the vector of paths before you proceed. Actually, since this tutorial does not run via your normal R console you won't see the downloaded files so you might choose to execute it directly in R instead!*

```{r congress, exercise=TRUE,exercise.lines=10}

```

```{r congress-solution}
paths<-read_html("https://www.congress.gov/congressional-record/116th-congress/browse-by-date") %>% html_nodes("td:nth-child(6) a") %>% html_attr("href")
paths<-paths[1:5]
filenames<-basename(paths)
paths<-paste0("https://www.congress.gov",paths)
mapply(download.file,paths,filenames,mode="wb")

# OR with a wrapper function:
download_files<-function(paths,filenames){
	download.file(url=paths,destfile=filenames,mode="wb")
}
mapply(download_files,paths,filenames)
```

