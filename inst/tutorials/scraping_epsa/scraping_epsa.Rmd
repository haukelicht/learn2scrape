---
title: "Scraping EPSA"
output: 
  learnr::tutorial:
    progressive: false
runtime: shiny_prerendered
---

```{r setup, include=FALSE}
library(learnr)
library(dplyr)
library(rvest)
library(learnr)
library(tidyr)
knitr::opts_chunk$set(echo = FALSE)
authorpage<-system.file("extdata", "authorpage.html", package = "learn2scrape") %>% read_html()
titlepage<-system.file("extdata", "titlepage.html", package = "learn2scrape") %>% read_html()
authors<-html_nodes(authorpage,".card__subtitle") %>% html_text(trim=T)
institutions<-html_nodes(authorpage, ".card__copy--authors-wrap div") %>% html_text(trim=T)
participants<-data.frame(authors,institutions,stringsAsFactors = F) %>% filter(duplicated(authors)==F)
rm(institutions,authors)

page<-read_html("https://app.oxfordabstracts.com/events/772/programme-builder/view/sort/title?view=published")

title_select<-".card__subtitle--program-abstract-title p"
time_select<-".card__datum--programme-time"
author_select<-".card__copy--authors-wrap"
institution_select<-".card__copy--affiliations"

titles<-page %>% html_nodes(title_select) %>% html_text(trim=T) 
authors<-page %>% html_nodes(author_select) %>% html_text(trim=T)
time<-page %>% html_nodes(time_select) %>% html_text(trim=T) 
institutions<-page %>% html_nodes(institution_select) %>% html_text(trim=T)
epsa_paper<-data.frame(titles, authors, time, institutions,stringsAsFactors = F)
```

## Introduction

This is an exercise for scraping the program of the [2019 EPSA Conference](https://www.epsanet.org/conference-2019/){target="_blank"}. The exercise format assumes you already have some experience with HTML - so just try your best and ask for hints or refer back to the other exercises if things do not work out.

For this session, I have pre-loaded the `rvest`, `tidyr` and `dplyr` packages, so remember to load them if you try to do this in your own R session. Due to the structure of the course, you unfortunately have to repeat steps like parsing the page multiple times.

## Scraping the conference program

### First steps

Proceeding as before, define the [starting url](https://app.oxfordabstracts.com/events/772/programme-builder/view/sort/title?view=published){target="_blank"} and parse it into a new object.

```{r generate-urls, exercise=TRUE}
"https://app.oxfordabstracts.com/events/772/programme-builder/view/sort/title?view=published"
```

```{r generate-urls-solution}
url<-"https://app.oxfordabstracts.com/events/772/programme-builder/view/sort/title?view=published"
page<-read_html(pageurl)
```

### Extracting information

There is a number of things we might be interested in:

- **paper titles**
- **authors**
- **sections** (based on the time)
- **institutions**
- abstracts
- potentially, other thinks like time slots...

Try to find CSS selectors for the bold items! We'll worry about the abstracts a bit later.

```{r selector-practice, echo=FALSE}
quiz(caption= "Quiz about selectors",
question("What is the CSS selector for the title of the paper? There are multiple selectors that work so if SelectorGadget gives you a different answer, just try out the ones below to see if they also work.",
  answer("p title"),
  answer(".card__subtitle--program-abstract-title p", correct=T),
  answer("title"),
  answer(".card__copy--authors-wrap"),
  allow_retry=TRUE
),
question("What is the CSS selector for the location and time of the panel",
  answer(".card__datum--programme-time",correct=T),
  answer(".time"),
  answer("time"),
  answer(".card__copy--authors-wrap"),
  allow_retry=TRUE
),
question("What is the CSS selector for the paper authors?",
  answer(".card"),
  answer("author"),
  answer(".card__copy--affiliations"),
  answer(".card__copy--authors-wrap",correct=T),
  allow_retry=TRUE
),
question("What is the CSS selector for the authors' institution?",
  answer(".card__affiliations"),
  answer(".card__subtitle--program-abstract-title p"),
  answer(".card__copy--affiliations",correct=T),
  answer(".affiliations"),
  allow_retry=TRUE
))
```



Now, parse the page again and extract the information (titles, location and time, authors and affiliations) from the parsed HTML page. Combine them into a new dataframe called `epsa_paper` using the `data.frame()` command. You can then inspect the data frame we downloaded.

*Tip: You can include trim=T inside html_text() to clean the results. Also, I recommend defining the CSS Selectors in a variable, rather than including it directly in* `html_nodes()`. *That way, you can re-use your code more easily.*

If you are not online and cannot scrape the page, use the object `titlepage` as parsed html page (that is, as result of `read_html()`).

```{r extraction,exercise=TRUE,exercise.lines=15}





```

```{r extraction-solution,eval=F}
page<-read_html("https://app.oxfordabstracts.com/events/772/programme-builder/view/sort/title?view=published")

title_select<-".card__subtitle--program-abstract-title p"
time_select<-".card__datum--programme-time"
author_select<-".card__copy--authors-wrap"
institution_select<-".card__copy--affiliations"

titles<-page %>% html_nodes(title_select) %>% html_text(trim=T) 
authors<-page %>% html_nodes(author_select) %>% html_text(trim=T)
time<-page %>% html_nodes(time_select) %>% html_text(trim=T) 
institutions<-page %>% html_nodes(institution_select) %>% html_text(trim=T)
epsa_paper<-data.frame(titles, authors, time, institutions,stringsAsFactors = F)
epsa_paper
```


## Adding author information

If you had a look at the author and institution information in the last section, it is not very intuitive. Instead of being listed by author, each author has a footnote with their institution - this is difficult when we need to disentangle papers where multiple authors come from the same institution.

While we could match this in R, we will make our lives a bit easier and practice scraping a bit more: We go to the [author page](https://app.oxfordabstracts.com/events/772/programme-builder/view/sort/author?view=published){target="_blank"} and scrape the names and institutions of everyone who attended EPSA. If you are not online and cannot scrape the page, use the object `authorpage` as parsed html page (that is, as result of `read_html()`). Then, combine them into a data frame.

*On an additional note: Some of our colleagues are registered with several accounts. If you want to make sure you do not create duplicates, just filter by applying the* `duplicated()` *command to the authors.*

```{r authors, exercise=TRUE,exercise.lines=5}



```
```{r authors-solution}
page<-read_html("https://app.oxfordabstracts.com/events/772/programme-builder/view/sort/author?view=published")
authors<-html_nodes(page,".card__subtitle") %>% html_text(trim=T)
institutions<-html_nodes(page, ".card__copy--authors-wrap div") %>% html_text(trim=T)
participants<-data.frame(authors,institutions,stringsAsFactors = F) %>% filter(duplicated(authors)==F)
```

### Matching authors and papers

So, how do we match the two? This is more a data management than a scraping question, so there are of course multiple ways. It is also a bit part of the grunt-work so feel free to move on to the next section instead. In fact, if you do not have much experience using `tidy` or `dplyr`, I recommend to skip this section because I won't be able to go into all of the details and this is really specific to the EPSA case.

If you do want to do the matching, I recommend using `separate()` from the tidyr package: it separates a single character vector into multiple columns of a data frame, based on a [regular expression](https://stringr.tidyverse.org/articles/regular-expressions.html){target="_blank"}. This way, we separate the author column into individual authors. Note that you have to provide a list of column names - I decided to just go for authors and numbers and just tried out until I saw that six would be enough... 

If you want, you can try to find a fitting regular expression here. I have pre-filled everything except the regular expression that you need to add as `sep`:

```{r regex,exercise=TRUE}
separate(epsa_paper,col=authors,into=c("author1","author2","author3","author4","author5","author6"),sep="")
```

```{r regex-solution}
separate(epsa_paper,col=authors,into=c("author1","author2","author3","author4","author5","author6"),sep="\\s*\n+\\s*\n*\\s*")
```


The next step is to reshape the data frame into a long format using `gather()`: each paper will appear multiple times with each author getting her or his own row. Then, we can drop lines with empty authors and match the institutions onto the authors, e.g. using a `left_join()`.

I have pre-loaded a data frame with the participants and their institutions (called `participants` with the variables `authors` and `institutions`) into your environment.

```{r merging, exercise=TRUE,exercise.lines=10}
epsa_paper<-epsa_paper %>% mutate(author_raw=authors) %>% 
separate(col="authors",into=c("author1","author2","author3","author4","author5","author6"),
  sep="\\s*\n+\\s*\n*\\s*") 
epsa_df_long<- epsa_paper %>% select(titles,author1:author6,time) %>% gather("number","authors",-c(titles,time)) %>%
  filter(is.na(authors)==F)
epsa_authorwise<-epsa_df_long %>% left_join(participants)
```

```{r merging-solution}
epsa_paper<-epsa_paper %>% mutate(author_raw=authors) %>% 
separate(col="authors",into=c("author1","author2","author3","author4","author5","author6"),
  sep="\\s*\n+\\s*\n*\\s*") 
epsa_df_long<- epsa_paper %>% select(titles,author1:author6,time) %>% gather("number","authors",-c(titles,time)) %>%
  filter(is.na(authors)==F)
epsa_authorwise<-epsa_df_long %>% left_join(participants)
```

## Looping through the abstracts

Getting the abstracts is a bit more complicated. Since they are not on the page we scraped, we will have to collect and follow the links to the details of each paper.

Unfortunately, SelectorGadget seems to do a bad job here - try it yourself 

Let us start by just extracting all links contained on the page - for that, just run the first line which selects all elements with an a-tag, that is, all links.

```{r alllinks,exercise=TRUE}
titlepage %>% html_nodes("a") %>% html_attr("href") 
```

Seems like there are some false matches. We have two options how to exclude them:

a) we sort them afterwards by only keeping links that confirm to a certain pattern
b) we sort them beforehand by modifying our CSS selector to only include certain patterns

Here, we go for the second option. First, try out which of the links are correct?

```{r complicatedpractice, echo=FALSE}
quiz(question("Which links lead to the pages with the paper abstracts?",
  answer("those starting with stages", correct=T),
  answer("those that are short"),
  answer("those starting with events"),
  allow_retry=TRUE
))
```

Great, so we know what to do. Luckily, we can use [more specific selectors](https://www.w3.org/TR/selectors-3/#attribute-substrings) to select exactly those nodes we want. There are basically three important such specific selectors: 

<style>
table, th, td {
  padding: 15px;
}
</style>

<table style="width:100%" >
<caption>Substring CSS Selectors</caption>
<tr>
<th>Format</td>
<th>Use</td>
<th>Example</td>
</tr>
<tr>
<td>&lsqb;attribute*=value&rsqb;</td>
<td>Matches elements with an attribute that contains a given value</td>
<td>a&lsqb;href*="pressrelease"&rsqb;</td>
</tr>
<tr>
<td>&lsqb;attribute^="value"&rsqb;</td>
<td>Matches elements with an attribute that starts with a given value</td>
<td>a&lsqb;href*="/press/"&rsqb;</td>
</tr>
<tr>
<td>&lsqb;attribute&dollar;="value"&rsqb;</td>
<td>Matches elements with an attribute that ends with a given value</td>
<td>&lsqb;href$=".pdf"&rsqb;</td>
</tr>
</table>



So, if we use `[href^=value]`, we only receive links that start with a certain value.


```{r abstracts,exercise=TRUE}

```


```{r abstracts-solution}
paper_links<-page %>% html_nodes("a[href^='/stages']") %>% html_attr("href")
get_text<-function(url,node="div.card__subtitle+ div"){
  html<-read_html(url)
  html_nodes(html,node) %>% html_text()
}

```



Now, it is time to scrape the abstracts.


## Wrap-up

Congratulations! You have scraped the abstracts of this year's EPSA conference. I hope you enjoyed it - we will work with them tomorrow during the text analysis class.

As mentioned before, [let me know](mailto:gessler@ipz.uzh.ch) via e-mail or in person if you have any suggestions how to make this tutorial more useful.
